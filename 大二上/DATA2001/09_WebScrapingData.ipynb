{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 9 (in Wk 10): Web Scraping Data\n",
    "\n",
    "## Introduction\n",
    "In this tutorial, you will get practical exposure to web scraping techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXERCISE 1 - Scraping Data from a Website\n",
    "\n",
    "In the lecture, we demonstrated how to extract data from a given website using Unix command line tools. In this tutorial, we will learn how to do so with Python.\n",
    "\n",
    "We will use the following Python libraries:\n",
    "- **Request**         for interacting with websites and web services\n",
    "- **Beautiful Soup**  for webpage parsing\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to load data from the following website that makes available historic data about the convict transportations to Australia in the late 18th and 19th century from the British Convict Transportation Register: https://convictrecords.com.au\n",
    "\n",
    "Let's start by having a look at the actual webpage.\n",
    "The following code requests the webpage for **the ship 'Batavia'** from this ConvictRecords website (the given URL scheme is specific for this website):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "webpage_source = requests.get(\"https://convictrecords.com.au/ships/batavia/\").text\n",
    "print(webpage_source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What you see above as the output of this request is the raw webpage source code.\n",
    "This is normally parsed and rendered by a web browser as a nice visual webpage.\n",
    "\n",
    "The language in which this webpage is written, is called **HTML** (the *Hypertext Markup Language*).\n",
    "In order to extract data from a webpage, we need at least some understanding of HTML, and in particularm that it is describing a tree-like structure of content elements which are rendered by a web browser.\n",
    "\n",
    "We can interpret the content of this webpage also inside a program by using a so-called **HTML parser**.\n",
    "For Python, there are several different options. In this tutorial, we are using a parser called *Beautiful Soup*. This parser allows a Python program to extract content of a webpage or a web service by finding specific HTML tags and giving access to the content inside.\n",
    "\n",
    "Have a look at the following Python code. It shows how the BeautifulSoup library is used to parse the webpage source code and then demonstrates different navigational commands of that library to identify specific tags within a (complex) page:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "page_content = BeautifulSoup(webpage_source, 'html5lib')\n",
    "\n",
    "# Example 1: print the title element of the page content\n",
    "print(\"Example 1: get a specific HTML element - such as the page title\")\n",
    "print(page_content.title)\n",
    "\n",
    "# Example 2: print just the text of the title element using the 'text' operator\n",
    "print(\"\\nExample 2: get HTML element's text content\")\n",
    "print(page_content.title.string)\n",
    "\n",
    "# Example 3: navigate along a tag sequence path and print content\n",
    "print(\"\\nExample 3: navigate along an element path to some content inside the page\")\n",
    "print(page_content.body.div['id'])\n",
    "\n",
    "# Example 4: find ALL hyperlinks on the page (anywhere on the page, just print URL)\n",
    "print(\"\\nExample 4: find_all() URLs of hyperlinks on this webpage\")\n",
    "for link in page_content.find_all(\"a\"):\n",
    "    if (link.has_attr('href')):  \n",
    "        print(link.get('href'));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1a:\n",
    "\n",
    "Ok, now that we have learned a bit about how to navigate in a webpage, let's get to work:\n",
    "\n",
    "What we would like to do in this exercise is to extract the ship voyage information from this page.\n",
    "\n",
    "When you scroll around the HTML code, you will see a around the last third of the source code an element\n",
    "> &lt;div id=\"ship\"&gt; \n",
    "\n",
    ">    &lt;h1&gt;Batavia Voyages to Australia&lt;/h1&gt;\n",
    " \n",
    "which describes the ship Batavia and its voyage to Australia in 1817 in a few more details.\n",
    "\n",
    "Lets use Python to extract just this part.\n",
    "To do so, we use the Beautiful Soup library to search for an HTML tag with id=\"ship\", extract it from the page into a variable *shipdetails*, and print it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "page_content = BeautifulSoup(webpage_source, 'html5lib')\n",
    "shipdetails  = page_content.find(id=\"ship\")\n",
    "print(shipdetails)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks already like some useful information, but there is still a lot of HTML formatting code included here.\n",
    "The **text** function allows us to just print all the printable text inside an HTML code fragment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(shipdetails.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, we are close. Now lets extract indiviudal parts of this text into some variables. To do so, we need to analyse and parse the actual HTML structure in which this text is contained.\n",
    "\n",
    "Checking back with the HTML fragment, we note that the ship name is used as alternative text for the image of the ship. We can extract this with an **path expression** that looks for the first **img** tag in the ship details, and then extracts the value of its **alt** attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shipname = shipdetails.img['alt']\n",
    "print(shipname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a subheading (**h3**) in the text that tells us the sailing date.\n",
    "This time, the text we are interested in is not an attribute of the element, but rather the text **string** inside the element. \n",
    "We do this as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "voyage = shipdetails.h3.string\n",
    "print(voyage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also can get the URL for the subpage with the details of that voyage from this h3 element:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "voyagedetails = shipdetails.h3.a[\"href\"]\n",
    "print(voyagedetails)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's get the information about when the ship arrived somewhere. This is a bit tricky because this part of the webpage has mixed content of a **h3** heading followed just by text without an enclosing HTML tag. If you draw the structure of our content fragment as a tree, you see that this text sits on the same level under the enclosing **div** element than the **h3** title in front of it. In the terminology of a tree, this is a **sibling** (the next sibling) to the h3-heading. \n",
    "\n",
    "So we need to tell the BeautifulSoup library to navigate along the HTML content exactly this way:\n",
    "From the **div** element (pointed to by the *shipdetails* variable) on to the **h3** heading element, then to its next  sibling content element which is the text about the arrival date:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "arrivaldate = shipdetails.h3.next_sibling.string\n",
    "print(arrivaldate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code is actually a shortcut. The h3-heading and the text are actually contained in a list further inside the HTML tree. The complete correct path to this element is shown below - but luckily BeautifulSoup allows us to use a shortcut, interpreting each dot-stop (.) as a 'next level or any level below' direction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Arrival date found along the complete HTML path:\")\n",
    "arrivaldate = shipdetails.div.ul.li.h3.next_sibling.string\n",
    "print(arrivaldate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seems to be some extra whitespace arount this arrival text; this can be removed using the **strip()** function on text.\n",
    "\n",
    "Ok, putting this all together as just one code, we get the following web scraping program for Ship details.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# retrieve the source code of the webpage\n",
    "page_html = requests.get(\"https://convictrecords.com.au/ships/batavia/\").text\n",
    "\n",
    "# parse the HTML content of the webpage and extract the ship's details part\n",
    "page_content = BeautifulSoup(page_html, 'html5lib')\n",
    "shipdetails = page_content.find(id=\"ship\")\n",
    "\n",
    "# extract selected information from the shipdetails, based on page structure\n",
    "shipname = shipdetails.img['alt']\n",
    "voyage = shipdetails.h3.text\n",
    "voyagedetails = shipdetails.h3.a[\"href\"]\n",
    "arrivaldate = shipdetails.h3.next_sibling.string\n",
    "\n",
    "# print out retrieved values\n",
    "print(\"Ship Name:  \" + shipname)\n",
    "print(\"Voyage:     \" + voyage)\n",
    "print(\"Voyage URL: \" + voyagedetails)\n",
    "print(arrivaldate.strip()) #remove extra whitespace from text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more documentation about functions available in BautifulSoup, see here:\n",
    "https://www.crummy.com/software/BeautifulSoup/bs4/doc/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1b:\n",
    "\n",
    "What happens if a ship did more than just a single voyage?\n",
    "\n",
    "In this case, its webpage would show a list of voyages - and our processing would need to be adjusted to be able to work with those lists.\n",
    "\n",
    "Let's see on how this work on the example of the ship 'Adelaide' which did 3 voayges to Australia between 1849 and 1863:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# retrieve the source code of the webpage\n",
    "page_html = requests.get(\"https://convictrecords.com.au/ships/adelaide/\").text\n",
    "\n",
    "# parse the HTML content of the webpage and extract the ship's details part\n",
    "page_content = BeautifulSoup(page_html, 'html5lib')\n",
    "shipdetails = page_content.find(id=\"ship\")\n",
    "print(shipdetails)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the format of the ConvictRecords web page is such that different voyages for the same ship are listed each with its own **h3** element followed by some text and links.\n",
    "Our code above would so far only retrieve the first voyage though.\n",
    "We need to adjust it such that it also works with multiple **h3** elements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# extract selected information from the shipdetails, based on page structure\n",
    "shipname = shipdetails.img['alt']\n",
    "voyages = shipdetails.find_all(\"h3\") # this finds ALL h3 elements describing voyages\n",
    "voyage = []\n",
    "voyurl = []\n",
    "voyarrival = []\n",
    "for v in voyages:\n",
    "    voyage.append(v.text)\n",
    "    voyurl.append(v.a[\"href\"])\n",
    "    voyarrival.append(v.next_sibling.string.strip())\n",
    "\n",
    "# print out retrieved values\n",
    "print(\"Ship Name: \" + shipname)\n",
    "print(\"The \" + shipname + \" made \" + str(len(voyage)) + \" voyages to Australia.\")\n",
    "#print(voyage)\n",
    "for i in range(len(voyages)):\n",
    "    print(\"Voyage \"+str(i+1)+\": \" + voyage[i])\n",
    "    print(\"     URL: \" + voyurl[i])\n",
    "    print(\"     \"+voyarrival[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YOUR TASK: Extract the passenger list from the voyage of the Glory\n",
    "\n",
    "In the lecture, we did briefly demonstrate how to extract a passenger list from a HTML table on a webpage on the ConvictRecords site. We used Unix tools there - your task here in this tutorial is to do the same with Python.\n",
    "\n",
    "The **table** tag in HTML has the following general structure:\n",
    "\n",
    "   tag names           | description\n",
    "   :------------------:|:------------------------------------------------------\n",
    "   **table**           | start tag of a HTML table\n",
    "   **tr** ... **/tr**  | start and end tags of a table *row*\n",
    "   **th** ... **/th**  | optional: start and end of a *header* field\n",
    "   **td** ... **/td**  | td tags encapsulate a *single data field* within a table\n",
    "   **/table**          | end tag of a table\n",
    "   \n",
    "\n",
    "Extract the *passenger list* from the **1818 journey** of the **ship Glory**, with the following constraints:\n",
    " - Extract the data into a Python **list** variable called **passengers** which is either a **list of strings** or better, a  **list of dictionary** data structure\n",
    " - As we already found out above, the url to this voyage is:<br>\n",
    "   https://convictrecords.com.au/ships/glory/1818"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: replace the content of this cell with your Python web scraping solution\n",
    "raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scraping with Google Spreadsheets\n",
    "\n",
    "As a side note: If the data is given as a table or an item list, you can retrieve the structured data also in Google Spreadsheets using its **ImportHTML()** function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# login to Google Spreadsheet\n",
    "# Type into the first cell:\n",
    "# =ImportHTML(\"https://convictrecords.com.au/ships/glory/1818\", \"table\", 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note however that this **only works for table or list data** (so not for our initial example with the structured ship details), and that you would need to do so manually for each individual webpage. With a dataset of about 1200 different ship voyages, this is not really an option..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXERCISE 2: Data Cleansing\n",
    "\n",
    "Once we retrieved data, we typically need to do some cleansing steps. This is especially important for data scrapped from webpages because many websites are not written with providing consistent data for a data analysis task in mind."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Special Characters in Scrapped Data\n",
    "\n",
    "The first issue we have to be aware of is that the scrapped data can include all kinds of special characters, some of which have special meaning in our target data models. \n",
    "\n",
    "For example, CSV files use commas (\",\") or tabs as a delimiter character between fields. If the text within one of such fields contains a comma (or tab) itself, the CSV file cannot be parsed correctly anymore. To avouid this, we need to **quote** all texts that potentially contain those special characters in CSV. Luckily, the csv write function in python allows us to do so as we will see later.\n",
    "\n",
    "Another special character is the single quote (\"'\"). This one is used in SQL to encapsulate strings. But what if a string itself includes a single quote? we have then to *escape* it first.\n",
    "\n",
    "Let's have a look on whether we find any single quote in our current passenger list from the ship **\"Glory\"**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for p in passengers:\n",
    "    if (p['Convict'].find(\"'\") > 0): \n",
    "        print (p['Convict'])\n",
    "    if (p['Description'].find(\"'\") > 0): \n",
    "        print (p['Description'])\n",
    "    if (p['Convicted_At'].find(\"'\") > 0): \n",
    "        print (p['Convicted_At'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And indeed we seem to have one entry in this passenger list where a name with a aphostrophe is used. This is however the same character then the SQL single quote.\n",
    "\n",
    "Now luckily, the python libraries for working with CSV files and with PostgreSQL are robust enough and will handle these special characters gracefully (at least with the utility functions which we provide below, and if you use dictionaries as data structures). **In our case, we do not need to do anything at this stage.**\n",
    "\n",
    "In other cases however this might now require some pre-processing on your side such as escaping any single quotes or replacing them. It is also a good practice to use dictionary data structures rather than lists because they use attribute key names rather than relaying on positions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Data Cleansing\n",
    "\n",
    "For the next exercise, look at the 'Crimes' page on the ConvictRecords website:<br>\n",
    " https://convictrecords.com.au/crimes\n",
    "\n",
    "This page is a good example for potential inconsistencies on a webpage, which we would like to avoid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YOUR TASK: Check and Discuss Data Inconsistencies in 'Crimes' list\n",
    "\n",
    "- Write Python code to read the 'Crimes' page from the ConvictRecords website, and extract the crimes list (just textual would be fine):  https://convictrecords.com.au/crimes\n",
    "- Check the content of this webpage and checkfor any data inconsistencies.\n",
    "- Discuss with your class mates and tutor your findings - and how you would handle each case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: replace the content of this cell with your solution and discussion points\n",
    "raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXERCISE 3: Data Storage\n",
    "Lets finally look at how we can store the extracted data in either a CSV file or a database for further processing. \n",
    "\n",
    "For the **CSV output** use the **csv** library of Python.<br>\n",
    "It is documented here: https://docs.python.org/2/library/csv.html\n",
    "\n",
    "For the **PostgreSQL** database output, use the **psycopg2** library as we used already in Week 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3a: Writing Scraped Data to a CSV File\n",
    "\n",
    "The first obvious export format for structured data, which we retrieved from a webpage, is a CSV file. Depending on which data structure you have chosen during data scraping in Exercise 1b (as a list or as a dictionary), this uses either the normal *writer* or the *DictWriter* function of the **csv** library (cf. https://docs.python.org/2/library/csv.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YOUR TASK: Write Passenger List as a CSV File\n",
    "\n",
    "Output your passenger list from the **passengers** variable as CSV file.\n",
    " - import the **csv** library\n",
    " - open a CSV file with a name of your choice\n",
    " - if your data is in a list of strings, use the standard **writer** from the **csv** library;<br>\n",
    "   if your data is in dictionary objects, use the **DictWriter** from the **csv** library\n",
    " - make sure that you set the CSV delimiter to comma (',') and the quote char to \"<br>\n",
    "   **Note:** Setting a *quote char* is important because some of the texts in the data include commas; without quoting those texts correctly, the CSV file cannot be correctly parsed later because the 'comma' is also used as a delimiter between fields.\n",
    " - write a header row, followed by as many rows as you have passengers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: replace the content of this cell with your Python solution\n",
    "raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3b: Writing Scraped Data to a Database\n",
    "\n",
    "In the next step, we are looking into how to store the scraped data directly into a PostgreSQL database. To do so, we copy the following two utility functions from Week 4 to connect to PostgreSQL and to execute an SQL command there from Python (actually, pgquery() is an improved version of pgexec() from Week 4).\n",
    "\n",
    "Please edit in the following cell the variables **YOUR_UNIKEY** and **YOUR_PW** to match your own PostgreSQL login."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import psycopg2.extras\n",
    "\n",
    "def pgconnect():\n",
    "    # please replace <your_unikey> and <your_SID> with your own details\n",
    "    YOUR_UNIKEY = #'unikey'#'<your_unikey>'\n",
    "    YOUR_PW     = #'abcd1234'#'<your_SID>'\n",
    "    try: \n",
    "        conn = psycopg2.connect(host='soit-db-pro-2.ucc.usyd.edu.au',\n",
    "                                database='y18s1d2001_'+YOUR_UNIKEY,\n",
    "                                user='y18s1d2001_'+YOUR_UNIKEY, \n",
    "                                password=YOUR_PW) \n",
    "        print('connected')\n",
    "    except Exception as e:\n",
    "        print(\"unable to connect to the database\")\n",
    "        print(e)\n",
    "    return conn\n",
    "\n",
    "def pgquery( conn, sqlcmd, args=None, silent=False, returntype='tuple'):\n",
    "    \"\"\" utility function to execute some SQL query statement\n",
    "        it can take optional arguments (as a dictionary) to fill in for placeholder in the SQL\n",
    "        will return the complete query result as return value - or in case of error: None\n",
    "        error and transaction handling built-in (by using the 'with' clauses)\"\"\"\n",
    "    retval = None\n",
    "    with conn:\n",
    "        cursortype = None if returntype != 'dict' else psycopg2.extras.RealDictCursor\n",
    "        with conn.cursor(cursor_factory=cursortype) as cur:\n",
    "            try:\n",
    "                if args is None:\n",
    "                    cur.execute(sqlcmd)\n",
    "                else:\n",
    "                    cur.execute(sqlcmd, args)\n",
    "                if (cur.description != None ):\n",
    "                    retval = cur.fetchall() # we use fetchall() as we expect only _small_ query results\n",
    "                if silent != False:\n",
    "                    print(\"success: \" + silent)\n",
    "            except Exception as e:\n",
    "                if e.pgcode != None and not(silent):\n",
    "                    print(\"db read error: \")\n",
    "                print(e)\n",
    "    return retval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YOUR TASK: Store Passenger List in Database\n",
    "Your task is as follows:\n",
    "* Create a table in your PostgreSQL database that is suitable for the passenger data retrieved from above's webpage.\n",
    "* Insert the extracted information of the passengers of the Batavia in that table\n",
    "* Query your database for how many passengers you have\n",
    "* Advanced: query the database whether there are any passengers with the same name (any passenger name used more than once?);\n",
    "* More Advanced: what are the top-10 common first names (you would need to either store first and last name in separate columns, or you sue the SUBSTR() function in SQL to get the frist name from the full names)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: replace the content of this cell with your Python + SQL solution\n",
    "raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "source": [
    "# EXERCISE 4: Using Web APIs\n",
    "\n",
    "In the final exercise, we are looking at some examples on how to access web APIs which are specifically provided for program to retrieve data. The advantage is that the data is well defined - no distracting HTML tags in between.\n",
    "\n",
    "But the services uses two different formats - either JSON or XML.\n",
    "\n",
    "For **JSON**, we will use the standard language support in Python and its **request** library.<br>\n",
    "For **XML**, we will use the **lxml** parser library.\n",
    "\n",
    "### Example 1: U.S. Government Website Analytic API\n",
    "\n",
    "First some JSON example from the U.S. government website analytics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The number of people who visited a U.S. government website using Internet Explorer 6.0 in the last 90 days\n",
    "import requests\n",
    "response = requests.get(\"https://analytics.usa.gov/data/live/ie.json\")\n",
    "print(response.json()['totals']['ie_version']['6.0'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which version of Internet Explorer is used most at this moment when contacting the U.S. government website. For this, we need to look at the actual JSON response. For this it is helpful to have a 'pretty-print' of the corresponding JSON data which is returned by the analytics.use.gov website. The Python **json** library can do this for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The raw response from the U.S. government website\n",
    "import requests\n",
    "response = requests.get(\"https://analytics.usa.gov/data/live/ie.json\")\n",
    "\n",
    "import json\n",
    "print(json.dumps(response.json(), indent=4, sort_keys=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above's data, it seems that IE version 11.0 is currently the most popular version of internet explorer used.\n",
    "<br>Amazingly, there are still visits with IE 4.0 though..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: ABS Population Clock API\n",
    "\n",
    "The Australian Bureau of Statistics provides the following web API os a *Population Clock Web Service* which gives some statistics about the current Australian population. The meaning of the various fields are explained here: http://www.abs.gov.au/AUSSTATS/abs@.nsf/Latestproducts/1420.0.55.001Main%20Features2User%20Guide?opendocument&tabname=Summary&prodno=1420.0.55.001&issue=User%20Guide&num=&view="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "response = requests.get(\"http://www.abs.gov.au/api/demography/populationprojection\")\n",
    "print(json.dumps(response.json(), indent=4, sort_keys=True))\n",
    "\n",
    "population = response.json()['popNow']\n",
    "print(\"Current population in Australia: \"+str(population))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3: Google Web API\n",
    "Here's another example with parameters send to a web service:\n",
    "The Google Wep-API allows to convert a location address to a GPS location (and some information more). The following example looks up the GPS location of the School of IT building at \"1 Cleveland Street, Darlington, Australia\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Lookup of a given address via Google Wep-API:\n",
    "import requests\n",
    "base_url = 'http://maps.googleapis.com/maps/api/geocode/json'\n",
    "my_params= {'address': '1 Cleveland Street,Darlington,Australia','language':'en'}\n",
    "response = requests.get(base_url, params = my_params)\n",
    "results  = response.json()['results']\n",
    "x_geo    = results[0]['geometry']['location']\n",
    "print(x_geo['lng'], x_geo['lat'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YOUR TASK: Retrieve Geo-Location of Arrival Ports of Convict Ship 'Adelaide'\n",
    " - Where lies 'Van Diemen's Land'?<br>\n",
    "   Use the Google Web-Api to check for the *GPS location* of the landing locations of the first voyage of the convict transportation ship \"Adelaide\" (cf. Exercise 1b): **Port Phillip** and **Van Diemen's Land**\n",
    " - Also retrieve the 'formatted_address'.<br>\n",
    "   For this you might need to inspect first how the JSON response is structured: Do hence first a pretty-print of the corresponding JSON response data.\n",
    " - Tip: if you want to see a map for a given GPS location, try: https://www.latlong.net/\n",
    " - Discuss: How would you store this information in your relational database next to the passenger list information?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: replace the content of this cell with your Python solution\n",
    "raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 4: Web API returning XML\n",
    "\n",
    "Some web APIs return data in **XML** format.\n",
    "The easiest library to work with such kind of data in Python is called **lxml**.\n",
    "Its documentation can be found here:<br>\n",
    "http://lxml.de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###### In the \"Justice News\" RSS feed maintained by the Justice Department, the number of items published on a Friday\n",
    "from datetime import datetime\n",
    "from lxml import etree\n",
    "import requests\n",
    "url = 'https://www.justice.gov/feeds/opa/justice-news.xml'\n",
    "news= requests.get(url).content\n",
    "doc = etree.fromstring(requests.get(url).content)\n",
    "items = doc.xpath('//channel/item')\n",
    "dates = [item.find('pubDate').text.strip() for item in items]\n",
    "ts = [datetime.strptime(d[0:16], \"%a, %d %b %Y\") for d in dates]\n",
    "# for weekday(), 4 correspond to Friday\n",
    "print(len([t for t in ts if t.weekday() == 4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 5: GitHub API\n",
    "Some final more complex example, extracting some information from the meta-data of a GitHub repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# From the lecture slides: list of programming languages used in PostgreSQL according to GitHub repositories\n",
    "import requests, json\n",
    "endpoint= \"https://api.github.com/users/postgres/repos\"\n",
    "repos   = json.loads(requests.get(endpoint).text)\n",
    "\n",
    "from dateutil.parser import parse\n",
    "from collections     import Counter\n",
    "dates = [parse(repo[\"created_at\"]) for repo in repos]\n",
    "month_counts = Counter(date.month for date in dates)\n",
    "weekday_counts = Counter(date.weekday() for date in dates)\n",
    "\n",
    "last_5_repositories = sorted(repos,\n",
    "                             key=lambda r: r[\"created_at\"],\n",
    "                             reverse=True)[:5]\n",
    "last_5_languages = [repo[\"language\"]\n",
    "                    for repo in last_5_repositories]\n",
    "print(last_5_languages)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Web API References\n",
    "\n",
    "If you are further interested in exploring some web APIs, have a look at the following lists:\n",
    "\n",
    "101 Data Journalist Challenges\n",
    "https://github.com/stanfordjournalism/search-script-scrape\n",
    "\n",
    "Tutorial on how to use New York Times API (needs registration with NYT)\n",
    "https://stanford.edu/~vbauer/teaching/nyt.html\n",
    "\n",
    "NSW Public Transport Events (needs registration)\n",
    "https://opendata.transport.nsw.gov.au/dataset/public-transport-realtime-alerts-0\n",
    "\n",
    "Twitter web API (needs registration):\n",
    "https://developer.twitter.com/en/docs/basics/getting-started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "Books:\n",
    "- Seppe van den Broucke and Bart Baesens: \"Practical Web Scraping for Data Science\", Springer 2018. (available electroinically via USYD library)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of Tutorial. Many Thanks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
