{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Processing as Unstructured Data\n",
    "\n",
    "## EXERCISE: SMS spam filtering with naive Bayes\n",
    "\n",
    "[Adapted from http://radimrehurek.com/data_science_python/.]\n",
    "\n",
    "Other references:\n",
    "* http://sebastianraschka.com/Articles/2014_naive_bayes_1.html\n",
    "* http://zacstewart.com/2015/04/28/document-classification-with-scikit-learn.html\n",
    "* https://gist.github.com/zacstewart/5978000\n",
    "\n",
    "Other options:\n",
    "* http://scikit-learn.org/stable/datasets/#the-20-newsgroups-text-dataset\n",
    "* http://scikit-learn.org/stable/datasets/#rcv1-dataset\n",
    "\n",
    "### Download data from UCI ML data repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib\n",
    "import zipfile\n",
    "\n",
    "DATA_URI = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip'\n",
    "DATA_DIR = 'data'\n",
    "ARCHIVE_NAME = 'smsspamcollection.zip'\n",
    "FILE_NAME = 'SMSSpamCollection'\n",
    "\n",
    "# set up paths (in portable, OS-agnostic way)\n",
    "local_archive_path = os.path.join(DATA_DIR, ARCHIVE_NAME)\n",
    "local_file_path = os.path.join(DATA_DIR, FILE_NAME)\n",
    "\n",
    "# set up local data directory\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "# save file from DATA_URI to local_path\n",
    "urllib.request.urlretrieve(DATA_URI, local_archive_path)\n",
    "\n",
    "# extract content from archive\n",
    "z = zipfile.ZipFile(local_archive_path, 'r')\n",
    "z.extractall(DATA_DIR)\n",
    "z.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read and profile data using Pandas\n",
    "\n",
    "Pandas provides tools that streamline some of the data analysis and visualisation work we've done in previous exercises. \n",
    "\n",
    "[DataFrame](http://pandas.pydata.org/pandas-docs/stable/dsintro.html#dataframe) is the most commonly used data structure in Pandas. It is a 2-dimensional labeled data structure with columns of potentially different types. You can think of it like a spreadsheet or SQL table.\n",
    "\n",
    "Let's use it now to read and profile our spam data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     label                                            message\n",
      "0      ham  Go until jurong point, crazy.. Available only ...\n",
      "1      ham                      Ok lar... Joking wif u oni...\n",
      "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
      "3      ham  U dun say so early hor... U c already then say...\n",
      "4      ham  Nah I don't think he goes to usf, he lives aro...\n",
      "5     spam  FreeMsg Hey there darling it's been 3 week's n...\n",
      "6      ham  Even my brother is not like to speak with me. ...\n",
      "7      ham  As per your request 'Melle Melle (Oru Minnamin...\n",
      "8     spam  WINNER!! As a valued network customer you have...\n",
      "9     spam  Had your mobile 11 months or more? U R entitle...\n",
      "10     ham  I'm gonna be home soon and i don't want to tal...\n",
      "11    spam  SIX chances to win CASH! From 100 to 20,000 po...\n",
      "12    spam  URGENT! You have won a 1 week FREE membership ...\n",
      "13     ham  I've been searching for the right words to tha...\n",
      "14     ham                I HAVE A DATE ON SUNDAY WITH WILL!!\n",
      "15    spam  XXXMobileMovieClub: To use your credit, click ...\n",
      "16     ham                         Oh k...i'm watching here:)\n",
      "17     ham  Eh u remember how 2 spell his name... Yes i di...\n",
      "18     ham  Fine if thats the way u feel. Thats the way ...\n",
      "19    spam  England v Macedonia - dont miss the goals/team...\n",
      "20     ham          Is that seriously how you spell his name?\n",
      "21     ham    I‘m going to try for 2 months ha ha only joking\n",
      "22     ham  So ü pay first lar... Then when is da stock co...\n",
      "23     ham  Aft i finish my lunch then i go str down lor. ...\n",
      "24     ham  Ffffffffff. Alright no way I can meet up with ...\n",
      "25     ham  Just forced myself to eat a slice. I'm really ...\n",
      "26     ham                     Lol your always so convincing.\n",
      "27     ham  Did you catch the bus ? Are you frying an egg ...\n",
      "28     ham  I'm back &amp; we're packing the car now, I'll...\n",
      "29     ham  Ahhh. Work. I vaguely remember that! What does...\n",
      "...    ...                                                ...\n",
      "5544   ham           Armand says get your ass over to epsilon\n",
      "5545   ham             U still havent got urself a jacket ah?\n",
      "5546   ham  I'm taking derek &amp; taylor to walmart, if I...\n",
      "5547   ham      Hi its in durban are you still on this number\n",
      "5548   ham         Ic. There are a lotta childporn cars then.\n",
      "5549  spam  Had your contract mobile 11 Mnths? Latest Moto...\n",
      "5550   ham                 No, I was trying it all weekend ;V\n",
      "5551   ham  You know, wot people wear. T shirts, jumpers, ...\n",
      "5552   ham        Cool, what time you think you can get here?\n",
      "5553   ham  Wen did you get so spiritual and deep. That's ...\n",
      "5554   ham  Have a safe trip to Nigeria. Wish you happines...\n",
      "5555   ham                        Hahaha..use your brain dear\n",
      "5556   ham  Well keep in mind I've only got enough gas for...\n",
      "5557   ham  Yeh. Indians was nice. Tho it did kane me off ...\n",
      "5558   ham  Yes i have. So that's why u texted. Pshew...mi...\n",
      "5559   ham  No. I meant the calculation is the same. That ...\n",
      "5560   ham                             Sorry, I'll call later\n",
      "5561   ham  if you aren't here in the next  &lt;#&gt;  hou...\n",
      "5562   ham                  Anything lor. Juz both of us lor.\n",
      "5563   ham  Get me out of this dump heap. My mom decided t...\n",
      "5564   ham  Ok lor... Sony ericsson salesman... I ask shuh...\n",
      "5565   ham                                Ard 6 like dat lor.\n",
      "5566   ham  Why don't you wait 'til at least wednesday to ...\n",
      "5567   ham                                       Huh y lei...\n",
      "5568  spam  REMINDER FROM O2: To get 2.50 pounds free call...\n",
      "5569  spam  This is the 2nd time we have tried 2 contact u...\n",
      "5570   ham               Will ü b going to esplanade fr home?\n",
      "5571   ham  Pity, * was in mood for that. So...any other s...\n",
      "5572   ham  The guy did some bitching but I acted like i'd...\n",
      "5573   ham                         Rofl. Its true to its name\n",
      "\n",
      "[5574 rows x 2 columns]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"4\" halign=\"left\">message</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ham</th>\n",
       "      <td>4827</td>\n",
       "      <td>4518</td>\n",
       "      <td>Sorry, I'll call later</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spam</th>\n",
       "      <td>747</td>\n",
       "      <td>653</td>\n",
       "      <td>Please call our customer service representativ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      message                                                               \n",
       "        count unique                                                top freq\n",
       "label                                                                       \n",
       "ham      4827   4518                             Sorry, I'll call later   30\n",
       "spam      747    653  Please call our customer service representativ...    4"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "import pandas\n",
    "\n",
    "messages = pandas.read_csv(local_file_path, sep='\\t', quoting=csv.QUOTE_NONE,\n",
    "                           names=[\"label\", \"message\"])\n",
    "print(messages)\n",
    "\n",
    "# view aggregate statistics\n",
    "messages.groupby('label').describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text feature extraction\n",
    "\n",
    "- It's easy for humans to see the patterns that distinguish ham and spam messages. What kind of features could we feed to a machine learning algorithm?\n",
    "\n",
    "Feeding raw text data (e.g., \"Sorry, I'll call later\") to a machine learning algorithm would not be very useful. As a first step let's assume that there is a systematic difference between the words used for spam and ham.\n",
    "\n",
    "`scikit-learn` includes [several functions for creating feature vectors from text](http://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction). These first tokenise -- split strings into words (e.g., \"Sorry\", \"I'll\", \"call\", \"later\").\n",
    "\n",
    "Then then create feature vectors where indices correspond to a specific word and values correspond to the frequency of the corresponding word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3734, 8745)\n",
      "Feature names:\n",
      " ['00', '000', '000pes', '008704050406', '0089', '0121', '01223585236', '01223585334', '0125698789', '02']\n",
      "\n",
      "Feature words for row 0:\n",
      " ['Ok']\n",
      "\n",
      "Original string for row0:\n",
      " Ok\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "\n",
    "# First split in to train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(messages.message, messages.label, test_size=0.33,\n",
    "                                                    random_state=5) # so we get the same results\n",
    "\n",
    "# Fit and transform training to feature vectors of words\n",
    "v = CountVectorizer(binary=True, lowercase=False)\n",
    "X_train_bin = v.fit_transform(X_train)\n",
    "print(X_train_bin.shape)\n",
    "\n",
    "# scikit-learn vectorisers produce sparse array representations.\n",
    "# These store only observed features for each instance instead of a complete vector.\n",
    "# They're great for working with text data, but require a bit of work to inspect.\n",
    "\n",
    "# View feature names\n",
    "print('Feature names:\\n', v.get_feature_names()[:10])\n",
    "\n",
    "# View complete feature vector for row 0 as a list of words\n",
    "def iter_features(X, row, names):\n",
    "    for i in X[row].indices:\n",
    "        yield names[i]\n",
    "print('\\nFeature words for row 0:\\n', list(iter_features(X_train_bin, 0, v.get_feature_names())))\n",
    "\n",
    "# Compare to original data\n",
    "print('\\nOriginal string for row0:\\n', X_train.iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build pipelines and choose parameters\n",
    "\n",
    "`scikit-learn` includes pipeline functionality that makes it possible to specify and optimise a sequence of actions.\n",
    "\n",
    "Let's see whether the results in [Wang and Manning (2012)](http://nlp.stanford.edu/pubs/sidaw12_simple_sentiment.pdf) hold for the SMS spam data we're using here.\n",
    "\n",
    "Specifically we will compare multinomial naive Bayes to support vector machine with a degree-2 polynomial kernel. We will choose the best feature representation for both:\n",
    "* binary vs term frequency vs tfidf\n",
    "* unigrams vs bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MNB best params:\n",
      " {'tfidf__use_idf': True, 'vect__binary': True, 'vect__ngram_range': (1, 1)}\n",
      "\n",
      "SVM best params:\n",
      " {'tfidf__use_idf': False, 'vect__binary': True, 'vect__ngram_range': (1, 1)}\n",
      "\n",
      "MNB test result:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        ham       0.95      1.00      0.98      1586\n",
      "       spam       1.00      0.69      0.82       254\n",
      "\n",
      "avg / total       0.96      0.96      0.95      1840\n",
      "\n",
      "\n",
      "SVM test result:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        ham       0.97      1.00      0.98      1586\n",
      "       spam       0.99      0.78      0.87       254\n",
      "\n",
      "avg / total       0.97      0.97      0.97      1840\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# Pipeline for multinomial naive Bayes\n",
    "mnb = Pipeline([('vect', CountVectorizer(lowercase=False)),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('clf', MultinomialNB())\n",
    "               ])\n",
    "\n",
    "# Pipeline for polynomial support vector machine\n",
    "svm = Pipeline([('vect', CountVectorizer(lowercase=False)),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('clf', LinearSVC(C=0.1, penalty='l2'))\n",
    "               ])\n",
    "\n",
    "# Grid search parameters\n",
    "param_grid = [{'vect__binary': [True],\n",
    "               'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "               'tfidf__use_idf': [True, False]\n",
    "              },\n",
    "              {'vect__binary': [False],\n",
    "               'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "               'tfidf__use_idf': [True, False]\n",
    "              }\n",
    "             ]\n",
    "\n",
    "# Find best parameters for MNB and SVM\n",
    "gs_mnb = GridSearchCV(mnb, param_grid)\n",
    "gs_mnb.fit(X_train, y_train)\n",
    "print('\\nMNB best params:\\n', gs_mnb.best_params_)\n",
    "\n",
    "gs_svm = GridSearchCV(svm, param_grid)\n",
    "gs_svm.fit(X_train, y_train)\n",
    "print('\\nSVM best params:\\n', gs_svm.best_params_)\n",
    "\n",
    "# Print accuracy\n",
    "print('\\nMNB test result:\\n', classification_report(y_test, gs_mnb.predict(X_test)))\n",
    "print('\\nSVM test result:\\n', classification_report(y_test, gs_svm.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO Handling class imbalance with SVM\n",
    "\n",
    "- Which is better? Do we care about overall performance or just one of our classes? How does this compare to Wang and Manning's result?\n",
    "- The `fit()` method for `LinearSVC` includes the `class_weight` parameter with can help deal with imbalanced data. The \"balanced\" mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as `n_samples / (n_classes * np.bincount(y))`. Train a new SVM model with the best grid search parameters and `class_weight='balanced'`. Is this result better?\n",
    "- Is there a more appropriate scoring function we could pass to `GridSearchCV` (using `sklearn.metrics.make_scorer`)? Does this give different MNB or SVM results for the parameter grid above?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-fada65ba5f30>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# TODO: replace the content of this cell with your Python solution\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# TODO: replace the content of this cell with your Python solution\n",
    "raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OPTIONAL Generalisation, data size, features\n",
    "\n",
    "- Plot training vs generalisation error by number of features (using the `max_features` option to CountVectorizer). Should we use fewer features?\n",
    "- Plot training vs generalisation error by amount of training data. Would it be worth obtaining more training data?\n",
    "- What other features could we use?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-fada65ba5f30>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# TODO: replace the content of this cell with your Python solution\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# TODO: replace the content of this cell with your Python solution\n",
    "raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXERCISE: Forecasting movie gross with support vector regression\n",
    "\n",
    "Imagine you have an opportunity to invest in films after seeing descriptions. Perhaps you run a cinema and need to decide which films to show. If we could predict box office gross based on descriptions, then we'd have a good basis for investment decisions.\n",
    "\n",
    "In this exercise we'll predict movie gross with support vector regression. \n",
    "\n",
    "### Download reviews and movie gross data\n",
    "\n",
    "DBPedia converts Wikipedia pages into structured semantic web data. Let's use it to grab the data we need. DBpedia has a public SPARQL endpoint at http://dbpedia.org/sparql. Enter the following query and save as `CSV` under `Results Format` and click `Run Query`.\n",
    "\n",
    "#### Query\n",
    "```\n",
    "PREFIX category: <http://dbpedia.org/resource/Category:>\n",
    "PREFIX dbtype: <http://dbpedia.org/datatype/>\n",
    "PREFIX dcterms: <http://purl.org/dc/terms/>\n",
    "\n",
    "SELECT ?dburl ?title ?budget ?gross ?abstract\n",
    "WHERE { \n",
    " ?dburl dcterms:subject category:2013_films .\n",
    "\n",
    " ?dburl foaf:name ?title .\n",
    " FILTER(LANG(?title) = \"en\") .\n",
    "\n",
    " ?dburl dbo:budget ?budget .\n",
    " FILTER(xsd:float(?budget) > xsd:float(\"1.0E7\")) .\n",
    " FILTER(datatype(?budget) = dbtype:usDollar) .\n",
    "\n",
    " ?dburl dbo:gross ?gross .\n",
    " FILTER(datatype(?gross) = dbtype:usDollar) .\n",
    "\n",
    " ?dburl dbo:abstract ?abstract .\n",
    " FILTER(LANG(?abstract) = \"en\") .\n",
    " FILTER(fn:string-length(?abstract) >= xsd:int(140)) .\n",
    "} \n",
    "```\n",
    "\n",
    "This will return the box office gross and the Wikipedia abstract for English films from 2013 that had a USD budget of more than 10 million. This will be our training data. Upload to your `data` directory on Jupyter Hub and rename to `movies.2013.csv`.\n",
    "\n",
    "Run the same query with `category:2014_films` to get test data. Upload to your `data` directory on Jupyter Hub and rename to `movies.2014.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "TRAIN_NAME = 'movies.2013.csv'\n",
    "TEST_NAME = 'movies.2014.csv'\n",
    "\n",
    "def read_movies(path):\n",
    "    data = []\n",
    "    target = []\n",
    "    for d in csv.DictReader(open(path)):\n",
    "        data.append(d['abstract'])\n",
    "        target.append(float(d['gross']))\n",
    "    return data, target\n",
    "\n",
    "train_path = os.path.join('data', TRAIN_NAME)\n",
    "X_train, y_train = read_movies(train_path)\n",
    "print(len(X_train), len(y_train))\n",
    "\n",
    "test_path = os.path.join('data', TEST_NAME)\n",
    "X_test, y_test = read_movies(test_path)\n",
    "print(len(X_test), len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select parameters for support vector regression\n",
    "\n",
    "Let's see whether the results in [Kogan et al. (2009)](http://www.cs.cmu.edu/~nasmith/papers/kogan+levin+routledge+sagi+smith.naacl09.pdf) hold for the movie gross data we're using here.\n",
    "\n",
    "Specifically we will choose the best feature representation for both:\n",
    "* binary vs term frequency vs tfidf\n",
    "* unigrams vs bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "\n",
    "svr = Pipeline([('vect', CountVectorizer(lowercase=True)),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('clf', LinearSVR(epsilon=0.1))\n",
    "               ])\n",
    "\n",
    "# Grid search parameters\n",
    "param_grid = {'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "              'vect__binary': [True, False],\n",
    "              'tfidf__use_idf': [True, False],\n",
    "              'tfidf__sublinear_tf': [True, False],\n",
    "              'clf__C': [1e8, 1e9, 1e10],\n",
    "             }\n",
    "\n",
    "# Find best parameters\n",
    "gs_svr = GridSearchCV(svr, param_grid)\n",
    "gs_svr.fit(X_train, y_train)\n",
    "print('Grid search mean and stdev:\\n')\n",
    "for params, mean_score, scores in gs_svr.grid_scores_:\n",
    "    print(\"{:0.3f} (+/-{:0.03f}) for {}\".format(\n",
    "            mean_score, scores.std() * 2, params))\n",
    "print('\\nSVR best params:\\n', gs_svr.best_params_)\n",
    "print('\\nSVR r-squared on training data:\\n', gs_svr.score(X_train, y_train))\n",
    "print('\\nSVR r-squared on test data:\\n', gs_svr.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO Evaluation and discussion\n",
    "\n",
    "- According to the 95% prediction interval, how close will our predictions be to the actual value? What if we calculate over the test data instead?\n",
    "- Draw a residual plot with training data in blue and test data in green (like we did in week 9). Is a linear model appropriate for our data? Is prediction performance comparable to training performance?\n",
    "- How could we improve this experimental setup? We can use `gs_svr.steps` to access feature names (`vect.get_feature_names()`) and weights (`clf.coef_`) from the pipeline components respectively. Use Python `zip` function to combine and sort these. What do the highest-weighted features tell us about our experimental setup?\n",
    "- Would you use this model to pick investments?\n",
    "- Maybe we're not predicting the right thing. What derived values could we predict? If you have time to try them, are they better?\n",
    "- Is there other text data that might be a better predictor of box office returns?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: replace the content of this cell with your Python solution\n",
    "raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Natural language parsing with SpaCy\n",
    "\n",
    "[Adapted from https://nicschrading.com/project/Intro-to-NLP-with-spaCy/.]\n",
    "\n",
    "### Load English parser\n",
    "\n",
    "`parser = English()` will initialise the models for core English language processing.\n",
    "\n",
    "We'll use some text from the first chapter of Alice in Wonderland, which is available from Project Gutenberg.\n",
    "\n",
    "Perhaps enjoy reading while waiting for the models to load (may take a minute or two). ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-56fa20dc5495>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Set up spaCy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0men\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mEnglish\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mEnglish\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Our text data from Ch1 of Alice in Wonderland\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "# Set up spaCy\n",
    "from spacy.en import English\n",
    "parser = English()\n",
    "\n",
    "# Our text data from Ch1 of Alice in Wonderland\n",
    "alice = \"\"\"\n",
    "Alice was beginning to get very tired of sitting by her sister on the\n",
    "bank, and of having nothing to do. Once or twice she had peeped into the\n",
    "book her sister was reading, but it had no pictures or conversations in\n",
    "it, \"and what is the use of a book,\" thought Alice, \"without pictures or\n",
    "conversations?\"\n",
    "\n",
    "So she was considering in her own mind (as well as she could, for the\n",
    "day made her feel very sleepy and stupid), whether the pleasure of\n",
    "making a daisy-chain would be worth the trouble of getting up and\n",
    "picking the daisies, when suddenly a White Rabbit with pink eyes ran\n",
    "close by her.\n",
    "\n",
    "There was nothing so very remarkable in that, nor did Alice think it so\n",
    "very much out of the way to hear the Rabbit say to itself, \"Oh dear! Oh\n",
    "dear! I shall be too late!\" But when the Rabbit actually took a watch\n",
    "out of its waistcoat-pocket and looked at it and then hurried on, Alice\n",
    "started to her feet, for it flashed across her mind that she had never\n",
    "before seen a rabbit with either a waistcoat-pocket, or a watch to take\n",
    "out of it, and, burning with curiosity, she ran across the field after\n",
    "it and was just in time to see it pop down a large rabbit-hole, under\n",
    "the hedge. In another moment, down went Alice after it!\n",
    "\"\"\"\n",
    "\n",
    "# Run the parser\n",
    "parsedData = parser(alice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print sentences and tokens\n",
    "\n",
    "The result splits out sentences and tokens within sentences. Let's have a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = list(parsedData.sents)\n",
    "for i, s in enumerate(sentences):\n",
    "    tokens = [t.text for t in s if t.text.strip()]\n",
    "    print(i, ' '.join(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print linguistic analysis\n",
    "\n",
    "The result also includes lemmas, part-of-speech (POS) tags, entity tags and grammatical dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('-'*79)\n",
    "print('{:<15} {:<15} {:<15} {:<15} {:<15}'.format('TOKEN', 'LEMMA', 'POS', 'ENTITY', 'DEPENDENCY'))\n",
    "print('-'*79)\n",
    "for token in sentences[0]:\n",
    "    if token.text.strip():\n",
    "        ent_tag = token.ent_iob_\n",
    "        if token.ent_type_:\n",
    "            ent_tag = '{}-{}'.format(token.ent_iob_, token.ent_type_)\n",
    "        dep_rel = token.dep_\n",
    "        if token.dep_ != 'ROOT':\n",
    "            dep_rel = '{}::{}'.format(token.dep_, token.head.text)\n",
    "        print('{:<15} {:<15} {:<15} {:<15} {:<15}'.format(token.text, token.lemma_, token.pos_, ent_tag, dep_rel))\n",
    "print('-'*79)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO Using parse data\n",
    "\n",
    "- Copy and paste the raw text for the first sentence into SpaCy's online demonstrator at https://spacy.io/demos/displacy. This gives a visual representation of the dependency graph.\n",
    "- Parse some news text (e.g., http://www.bbc.com/news/world-asia-china-36309063). Alternatively, view a parse using the demonstrator (e.g., https://spacy.io/demos/displacy?share=5036304037475277146). How could we use dependency relations to extract relationships from this data (e.g., `chairman-of<\"Mr Zhang\", \"National People's Congress Standing Committee\">`)?\n",
    "- How could we use SpaCY to include more linguistic analysis in our text classification and forecasting systems? If you have time, try some of these ideas (e.g., https://www.site.uottawa.ca/eng/school/publications/techrep/2007/TR-2007-12.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: replace the content of this cell with one idea about how to use SpaCY in our text classification systems\n",
    "raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
