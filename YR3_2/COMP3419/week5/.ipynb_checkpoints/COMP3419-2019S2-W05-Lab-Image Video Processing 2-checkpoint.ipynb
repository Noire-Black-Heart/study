{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Intensity based Object Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting: 119\n"
     ]
    }
   ],
   "source": [
    "# Setup for object tracking\n",
    "\n",
    "if not os.path.isdir(os.path.join(os.getcwd(), 'frames')):\n",
    "    os.mkdir(\"frames\")\n",
    "else:\n",
    "    print('frames already exists')\n",
    "\n",
    "if not os.path.isdir(os.path.join(os.getcwd(), 'composite')):\n",
    "    os.mkdir(\"composite\")\n",
    "else:\n",
    "    print('composite already exists')\n",
    "    \n",
    "framenumber = 0\n",
    "framectr = 0\n",
    "omovie = cv2.VideoCapture('ping_pang.mov')\n",
    "frame_height = omovie.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
    "frame_width = omovie.get(cv2.CAP_PROP_FRAME_WIDTH)\n",
    "\n",
    "# Extract the frames from original video\n",
    "while(1):\n",
    "    ret, frame = omovie.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    print('Extracting: %d' % framenumber)\n",
    "    clear_output(wait=True)\n",
    "    cv2.imwrite('frames/%d.tif' % framenumber, frame)\n",
    "    framenumber += 1\n",
    "omovie.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Find the object coordinate by averaging the foreground coordinates\n",
    "    - Function Input: \n",
    "            -         frame:   numpy array, the frame to be processed\n",
    "            -     threshold:           int, the threshold to segment the object\n",
    "\n",
    "    - Function Output:\n",
    "            - [object_x, object_y]: the coordinate of the object in the frame\n",
    "'''\n",
    "def findObj(frame, threshold):\n",
    "    ############# TO DO #############\n",
    "\n",
    "    return \n",
    "\n",
    "\n",
    "# Draw a circle on the image\n",
    "def drawbox(frame, centerx, centery, radius, color):\n",
    "    for y in range(centerx - radius, centerx + radius):\n",
    "        for x in range(centery - radius, centery + radius):\n",
    "            cx = 0 if x < 0 else frame.shape[0]-1 if x > frame.shape[0] - 1 else x\n",
    "            cy = 0 if y < 0 else frame.shape[1]-1 if y > frame.shape[1] - 1 else y\n",
    "            for i in range(3):\n",
    "                frame[cx][cy][i] = color[i]\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation Tips:\n",
    "* You can use np.hstack((img1, img2)) to horizontally stack two images into one\n",
    "* You can use 'cv2.putText(combined_img, text='Sample Text', org=(2200, 1000),fontFace=cv2.FONT_HERSHEY_DUPLEX, fontScale=5, thickness = 5, color=(0, 255, 0)' to put text on a frame\n",
    "* You can use 'cv2.line(img, (x1, y1), (x2, y2), (0, 0, 255), 2)' to draw a red line between two points (x1, y1) and (x2, y2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "framectr = framenumber - 1\n",
    "process_frame = 0\n",
    "\n",
    "foreground = 250 # Foreground Threshold for Segmentation\n",
    "# Store the coordinates found by intensity thresholding\n",
    "coordListX = list()\n",
    "coordListY = list()\n",
    "\n",
    "while process_frame <= framectr:\n",
    "    oframe = cv2.imread('frames/%d.tif' % process_frame)\n",
    "    print('Processing frame: %d, overall progress: %.2f %%' % (process_frame, process_frame/framectr*100))\n",
    "    clear_output(wait=True)\n",
    "    \n",
    "    # Change frame to grey scale\n",
    "    gframe = oframe.copy() # Grey scaled frame\n",
    "\n",
    "    # Load the saved frames sequentially\n",
    "    height = None\n",
    "    width = None\n",
    "    for y in range(gframe.shape[1]):\n",
    "        for x in range(gframe.shape[0]):\n",
    "            # Convert to gray scale\n",
    "            g = 0.212671 * gframe[x][y][2] + 0.715160 * gframe[x][y][1] + 0.072169 * gframe[x][y][0]\n",
    "\n",
    "            # Convert to binary\n",
    "            for i in range(3):\n",
    "                if g > foreground:\n",
    "                    gframe[x][y][i] = 255\n",
    "                else:\n",
    "                    gframe[x][y][i] = 0\n",
    "\n",
    "    # Get the initial state (object coordinates) from binary segmentation\n",
    "    coord = findObj(gframe, 128) # coord is the centre of mass\n",
    "                                 # coord[0] : y column\n",
    "                                 # coord[1] : x row\n",
    "            \n",
    "    # Draw a red dot in the centre of the segmented object\n",
    "    oframe = drawbox(oframe, int(coord[1]), int(coord[0]), 5, (0, 0, 255))\n",
    "    gframe = drawbox(gframe, int(coord[1]), int(coord[0]), 5, (0, 0, 255))\n",
    "    combined_img = np.hstack((oframe, gframe))\n",
    "    ####################################### TODO ###############################################\n",
    "    # 1. You need to perform the object segmentation results on 'gframe'                       #\n",
    "    # 2. You need to display the object moving trajectory on 'oframe'                          #\n",
    "    # 3. You need to display a text to determine whether the object is found or not in a frame #\n",
    "    # 4. Please see the lab sheet for reference                                                #\n",
    "    \n",
    "    \n",
    "\n",
    "    ####################################### END ################################################\n",
    "    cv2.imwrite('composite/composite%d.tif' % process_frame, combined_img)\n",
    "    if cv2.waitKey(30) & 0xff == ord('q'):\n",
    "        break\n",
    "    process_frame += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "out = cv2.VideoWriter('./happy_ping_pang.mov', cv2.VideoWriter_fourcc('M', 'J', 'P', 'G'), 10, (int(frame_width*2), int(frame_height)))\n",
    "while(1):\n",
    "    img = cv2.imread('composite/composite%d.tif' % count)\n",
    "    if img is None:\n",
    "        print('No more frames to be loaded')\n",
    "        break\n",
    "    clear_output(wait=True)\n",
    "    out.write(img)\n",
    "    count += 1\n",
    "    print('Saving video: %d%%' % int(100*count/framenumber))\n",
    "    \n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intensity based Object Tracking - Week5 Lab Exercise Submission\n",
    "You can now use the generate_results() function below to generate your outputs for submission. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    generate_results function is a helper function for you to generate\n",
    "    the output images of lab exercise submission\n",
    "    - Function Input: \n",
    "            -            wk:           int, indicates a specific week's lab exercise\n",
    "            -          name:           str, the name of the student\n",
    "            -           SID:           int, the SID of the student\n",
    "            -  output_video:           str, the path to output_video\n",
    "\n",
    "    - Function Usage:\n",
    "            - Supply all the arguments with the correct types and a result image\n",
    "              will be generated.\n",
    "    - Tips:\n",
    "            - You can right click the result image plot to save the image or \n",
    "              you can take a screenshoot for the submission.\n",
    "'''\n",
    "def generate_results(wk, name, SID, output_video):\n",
    "    cap = cv2.VideoCapture(output_video)\n",
    "    random_frames = []\n",
    "    if not cap.isOpened():\n",
    "        print('%s not opened' % output_video.split('/')[-1])\n",
    "        sys.exit(1)\n",
    "\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    x = cap.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
    "    y = cap.get(cv2.CAP_PROP_FRAME_WIDTH)\n",
    "    while x > 10:\n",
    "        x /= (x / 10)\n",
    "        y /= (y / 10)\n",
    "    \n",
    "    random_frames.append(random.randint(53, 63))\n",
    "    random_frames.append(random.randint(66, 76))\n",
    "    random_frames.append(random.randint(80, 94))\n",
    "        \n",
    "    fig, axs = plt.subplots(3, 1, figsize=(x,y))\n",
    "        \n",
    "    count = 0\n",
    "    output_count = 0\n",
    "    while(1):\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        if count in random_frames:\n",
    "            frame_m = frame.copy()\n",
    "            frame_R = frame[:,:,2]\n",
    "            frame_B = frame[:,:,0]\n",
    "            frame_m[:,:,2] = frame_B\n",
    "            frame_m[:,:,0] = frame_R\n",
    "            frame = np.uint8(frame_m)\n",
    "\n",
    "            axs[output_count].imshow(frame_m)\n",
    "            axs[output_count].text(0.5,-0.1, 'Composite frame: ' + str(count), size=12, ha=\"center\", transform=axs[output_count].transAxes)\n",
    "            axs[output_count].axis('off')\n",
    "            output_count+=1\n",
    "            \n",
    "            if output_count >= 3:\n",
    "                break\n",
    "        count+=1\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    fig.suptitle(\"Week %i Lab Exercise\\n %s SID:%i\"%(wk, name, SID),x=0.5,y=1.05)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the 'path_to_output' to the path where your composited video is located\n",
    "path_to_output = './happy_ping_pang.mov'\n",
    "generate_results(5, 'Your name', 12345567, path_to_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
